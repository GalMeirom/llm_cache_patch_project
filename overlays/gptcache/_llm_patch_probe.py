# overlays/gptcache/_llm_patch_probe.py
"""Probe module written by the LLM Cache Project overlay."""
LLM_PATCH_SENTINEL = True
